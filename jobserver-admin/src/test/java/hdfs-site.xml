<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <property>
        <name>dfs.nameservices</name>
        <value>newns</value>
    </property>
    <property>
        <name>dfs.ha.namenodes.newns</name>
        <value>nn1,nn2</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.newns.nn1</name>
        <value>10.5.20.10:8020</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.newns.nn2</name>
        <value>10.5.20.11:8020</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.newns.nn1</name>
        <value>10.5.20.10:50070</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.newns.nn2</name>
        <value>10.5.20.11:50070</value>
    </property>

    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://10.5.20.10:8485;10.5.20.11:8485;10.5.20.12:8485/newns</value>
        <description>10.5.20.10, 10.5.20.11, 10.5.20.12</description>
    </property>
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/data/bigdata/hadoop/jn</value>
    </property>

    <property>
        <name>dfs.client.failover.proxy.provider.newns</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>

    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>

    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/home/devops/.ssh/id_rsa</value>
    </property>
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.qjournal.write-txns.timeout.ms</name>
        <value>3600000</value>
    </property>
    <property>
        <name>dfs.qjournal.finalize-segment.timeout.ms</name>
        <value>3600000</value>
    </property>
    <property>
        <name>dfs.qjournal.start-segment.timeout.ms</name>
        <value>3600000</value>
    </property>
    <property>
        <name>ipc.client.connect.max.retries</name>
        <value>10</value>
        <description>Indicates the number of retries a client will make to establish
            a server connection.
        </description>
    </property>
    <property>
        <name>ipc.client.connect.retry.interval</name>
        <value>2000</value>
        <description>Indicates the number of milliseconds a client will wait for
            before retrying to establish a server connection.
        </description>
    </property>
    <property>
        <name>dfs.namenode.duringRollingUpgrade.enable</name>
        <value>true</value>
    </property>
    <!-- HA  }}} -->

    <property>
        <name>dfs.blocksize</name>
        <value>128m</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/data/bigdata/hadoop/nn</value>
    </property>
    <property>
        <name>dfs.balance.bandwidthPerSec</name>
        <value>104857600</value>
        <description>100M/sec</description>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/data/bigdata/hadoop/dn</value>
    </property>
    <property>
        <name>fs.permissions.umask-mode</name>
        <value>002</value>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.client.read.shortcircuit</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.domain.socket.path</name>
        <value>/data/hadoop/pid/dn.socket</value>
    </property>
    <property>
        <name>dfs.datanode.handler.count</name>
        <value>100</value>
    </property>
    <property>
        <name>dfs.datanode.max.transfer.threads</name>
        <value>8192</value>
    </property>
    <property>
        <name>dfs.datanode.failed.volumes.tolerated</name>
        <!--需要修改-->
        <value>0</value>
    </property>
    <property>
        <name>dfs.namenode.handler.count</name>
        <value>400</value>
        <description>This value usually is set to Math.log(N)*20, N = cluster size</description>
    </property>
    <property>
        <name>dfs.datanode.du.reserved</name>
        <!--需要修改-->
        <value>214748364</value>
    </property>
    <property>
        <name>dfs.hosts.exclude</name>
        <value>/opt/hadoop/etc/hadoop/dfs.hosts.exclude</value>
        <description>Names a file that contains a list of hosts that are
            not permitted to connect to the namenode.  The full pathname of the
            file must be specified.  If the value is empty, no hosts are
            excluded.</description>
    </property>
    <property>
        <name>fs.protected.directories</name>
        <value>/tmp,/user,/logs,/user/hbase,/user/hive,/user/hive/warehouse/dw.db,/test/a</value>
        <description>A comma-separated list of directories which cannot
            be deleted even by the superuser unless they are empty. This
            setting can be used to guard important system directories
            against accidental deletion due to administrator error. "/,/hive,/hbase"
        </description>
    </property>
    <property>
        <name>dfs.namenode.fs-limits.max-directory-items</name>
        <value>4194304</value>
    </property>

    <property>
        <name>dfs.datanode.fsdataset.volume.choosing.policy</name>
        <value>org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>

    <property>
        <name>dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction</name>
        <value>0.9</value>
    </property>
    <property>
        <name>dfs.datanode.balance.max.concurrent.moves</name>
        <value>50</value>
    </property>
    <property>
        <name>dfs.namenode.replication.considerLoad</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.image.compress</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.image.compression.codec</name>
        <value>org.apache.hadoop.io.compress.SnappyCodec</value>
    </property>
    <property>
        <name>ipc.server.listen.queue.size</name>
        <value>256</value>
    </property>
    <property>
        <name>dfs.namenode.decommission.interval</name>
        <value>900</value>
    </property>
    <property>
        <name>dfs.namenode.decommission.nodes.per.interval</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.client.socket-timeout</name>
        <value>120000</value>
    </property>
    <property>
        <name>dfs.namenode.audit.log.async</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.namenode.heartbeat.recheck-interval</name>
        <value>600000</value>
    </property>
    <property>
        <name>dfs.balancer.getBlocks.min-block-size</name>
        <value>104857600</value>
    </property>
    <property>
        <name>dfs.balancer.moverThreads</name>
        <value>1000</value>
    </property>
    <property>
        <name>dfs.balancer.dispatcherThreads</name>
        <value>1</value>
    </property>

    <property>
        <name>dfs.balancer.getBlocks.size</name>
        <value>10000000000</value>
        <description>getBlocks 10G </description>
    </property>

</configuration>